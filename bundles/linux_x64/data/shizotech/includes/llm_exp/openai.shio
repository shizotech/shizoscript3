
import curl;

class openai_api
{
	api_url = "";
	body = [stream=true, reasoning_effort="medium", max_tokens = 128000];
	
	on_reason = none;
	on_text = none;
	
	__init__(
		api_url, 
		model = ""
	) {
		this.api_url = api_url + "/v1/chat/completions";
		if(model)
			body.model = model;
	}
	
	ask(prompt)
	{
		body.messages.push_back(
			["role" = "user", "content" = prompt]
		);
		
		headers = [
		  "Content-Type" = "application/json"
		];
		
		cl = curl.curl();
		
		cl.start_stream("POST", api_url, body, headers, 0);

		final_answer = "";

		for (raw_chunk = cl.poll_stream()) {
			chunk = std.json(raw_chunk);
			
			delta = &chunk.choices[0].delta;
			final_answer += delta.content;
			
			if(delta.reasoning_content && std.is_function(on_reason))
				on_reason(delta.reasoning_content);
			if(delta.content && std.is_function(on_text))
				on_text(delta.content);
		}
		
		return final_answer;
	}
}

openai_api_request(prompt, sys_prompt = "", llm_url = "http://localhost:8000", default_output = 0)
{
	opi = openai_api(llm_url);
	
	if(!sys_prompt.empty()) {
		opi.body.messages.push_back(
			["role" = "developer", "content" = sys_prompt]
		);
	}
	
	if(default_output)
	{
		opi.on_reason = [](txt)
		{
			std.cout(txt);
		};
		opi.on_text = [](txt)
		{
			std.cout(txt);
		};
	}
	
	return opi.ask(prompt);
}