class llmchat
{
	messages = [];
	
	add(role, msg)
	{
		messages.push_back([role=role, content=msg]);
	}
}

class llmclient
{
	model 		= "default_big";
	gpt_net 	= shizonet.server("AI-AGENT");
	chat 		= llmchat();
	
	temperature 	= 0.4;
	top_p 			= 0.95;
	top_k 			= -1;
	
	max_tokens_stream = 16;
	
	__init__(sys_prompt = "You are a helpful AI Assistant. Do whatever the user requests.")
	{
		chat.add("system", sys_prompt);
	}
	
	text(prompt)
	{
		chat.add("user", prompt);	
		raw_prompt = get_chat_template();
		
		answer = "";
		token_count = raw_prompt.token_count;
		
		std.cout("\n\n");
		
		for(1) {
			result = run_text(raw_prompt.prompt, [
					max_tokens=max_tokens_stream,
					skip_special_tokens=1
				]);
			
			raw_prompt.prompt += result.answer;
			answer += result.answer;
			token_count += result.token_count;
			
			std.cout(" " + token_count + " ");
			std.cout(result.answer);
			
			if(result.finished)
				break;
		}
		
		std.cout("\n\n");
		
		answer = answer.substr(answer.find("</think>") + std.len("</think>"));
		
		chat.add("assistant", answer);
		return answer;
	}
	
	get_chat_template()
	{
		return run_cmd("llm_prompt_"+model, [messages=chat.messages]);
	}
	
	run_text(raw_prompt, options=0)
	{
		run_args = [
			prompt=raw_prompt,
			temperature=temperature,
			top_p=top_p,
			top_k=top_k
		];
		for(i = 0; i < options.size(); i++)
		{
			run_args[options.key(i)] = options[i];
		}	
		
		res = run_cmd("llm_text_" + model, run_args);
		
		return res;
	}
	
	run_cmd(cmd, params)
	{
		params.skip_special_tokens = 0;	
		res = 0;
		params.cmd = cmd;
		for(res == 0)
		{
			res = gpt_net.get("run_wq", params);

			if(res == 0)
			{
				std.sleep(500);
			}
		}
	
		return res;	
	}
}

llm_single_task(sys_prompt, prompt, options = [])
{
	tmp_client = llmclient(sys_prompt);
	if(options.has("model"))
		tmp_client.model = options.model;
	tmp_client.max_tokens_stream = 128000;
	return tmp_client.text(prompt);
}